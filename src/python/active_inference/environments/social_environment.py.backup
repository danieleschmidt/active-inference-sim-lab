"""
Social environment for multi-agent active inference research.

This module implements social dilemma scenarios to test active inference
agents in multi-agent settings with theory of mind capabilities.
"""

import numpy as np
from typing import Dict, Any, List, Tuple, Optional
from enum import Enum
import logging


class AgentType(Enum):
    """Types of agents in social environment."""
    COOPERATIVE = 0
    COMPETITIVE = 1
    ALTRUISTIC = 2
    SELFISH = 3


class SocialAction(Enum):
    """Social actions available to agents."""
    COOPERATE = 0
    DEFECT = 1
    SHARE = 2
    HOARD = 3


class SocialDilemmaEnvironment:
    """
    Multi-agent environment for studying social cognition in active inference.
    
    Implements various social dilemmas:
    - Prisoner's Dilemma
    - Public Goods Game
    - Resource Sharing
    - Trust Games
    """
    
    def __init__(self,
                 n_agents: int = 2,
                 game_type: str = "prisoners_dilemma",
                 max_rounds: int = 20,
                 reputation_system: bool = True,
                 communication: bool = False):
        """
        Initialize social dilemma environment.
        
        Args:
            n_agents: Number of agents
            game_type: Type of social game
            max_rounds: Maximum rounds per episode
            reputation_system: Whether agents track reputations
            communication: Whether agents can communicate
        """
        self.n_agents = n_agents
        self.game_type = game_type
        self.max_rounds = max_rounds
        self.reputation_system = reputation_system
        self.communication = communication
        
        # Agent states
        self.agent_types = [AgentType.COOPERATIVE] * n_agents
        self.agent_scores = np.zeros(n_agents)
        self.agent_reputations = np.ones((n_agents, n_agents)) * 0.5  # Initial neutral reputation
        
        # Game state
        self.round_count = 0
        self.episode_count = 0
        self.action_history = []
        self.outcome_history = []
        self.reputation_history = []
        
        # Game parameters
        self._setup_game_parameters()
        
        # Logging
        self.logger = logging.getLogger("SocialEnvironment")
    
    def _setup_game_parameters(self):
        """Setup payoff matrices and game-specific parameters."""\n        if self.game_type == \"prisoners_dilemma\":\n            # Payoff matrix: [own_cooperate, other_cooperate] -> reward\n            self.payoff_matrix = {\n                (SocialAction.COOPERATE, SocialAction.COOPERATE): (3, 3),\n                (SocialAction.COOPERATE, SocialAction.DEFECT): (0, 5),\n                (SocialAction.DEFECT, SocialAction.COOPERATE): (5, 0),\n                (SocialAction.DEFECT, SocialAction.DEFECT): (1, 1)\n            }\n            \n        elif self.game_type == \"public_goods\":\n            # Public goods game parameters\n            self.endowment = 10.0  # Initial endowment per agent\n            self.multiplier = 1.5  # Return multiplier for contributions\n            self.contribution_cost = 1.0\n            \n        elif self.game_type == \"resource_sharing\":\n            # Resource sharing parameters\n            self.total_resources = 100.0\n            self.sharing_bonus = 0.2  # Bonus for sharing\n            self.hoarding_penalty = 0.1\n            \n        elif self.game_type == \"trust_game\":\n            # Trust game parameters\n            self.initial_endowment = 10.0\n            self.trust_multiplier = 3.0\n            self.trustworthiness_threshold = 0.5\n    \n    def reset(self) -> List[np.ndarray]:\n        \"\"\"Reset environment and return initial observations for all agents.\"\"\"\n        self.round_count = 0\n        self.episode_count += 1\n        self.agent_scores = np.zeros(self.n_agents)\n        \n        # Reset reputation matrix to neutral\n        if self.reputation_system:\n            self.agent_reputations = np.ones((self.n_agents, self.n_agents)) * 0.5\n            # Agents don't have reputation for themselves\n            np.fill_diagonal(self.agent_reputations, 0.0)\n        \n        # Clear histories\n        self.action_history = []\n        self.outcome_history = []\n        self.reputation_history = []\n        \n        # Generate initial observations\n        observations = []\n        for agent_id in range(self.n_agents):\n            obs = self._get_observation(agent_id)\n            observations.append(obs)\n        \n        self.logger.info(f\"Social environment reset for episode {self.episode_count}\")\n        return observations\n    \n    def step(self, actions: List[int]) -> Tuple[List[np.ndarray], List[float], bool, Dict]:\n        \"\"\"\n        Execute one round of the social game.\n        \n        Args:\n            actions: List of actions for each agent\n            \n        Returns:\n            observations, rewards, done, info\n        \"\"\"\n        if len(actions) != self.n_agents:\n            raise ValueError(f\"Expected {self.n_agents} actions, got {len(actions)}\")\n        \n        self.round_count += 1\n        \n        # Convert to SocialAction enum\n        social_actions = [SocialAction(action) for action in actions]\n        \n        # Compute rewards based on game type\n        if self.game_type == \"prisoners_dilemma\":\n            rewards = self._compute_prisoners_dilemma_rewards(social_actions)\n        elif self.game_type == \"public_goods\":\n            rewards = self._compute_public_goods_rewards(social_actions)\n        elif self.game_type == \"resource_sharing\":\n            rewards = self._compute_resource_sharing_rewards(social_actions)\n        elif self.game_type == \"trust_game\":\n            rewards = self._compute_trust_game_rewards(social_actions)\n        else:\n            raise ValueError(f\"Unknown game type: {self.game_type}\")\n        \n        # Update scores\n        self.agent_scores += rewards\n        \n        # Update reputations\n        if self.reputation_system:\n            self._update_reputations(social_actions, rewards)\n        \n        # Record history\n        self.action_history.append(social_actions)\n        self.outcome_history.append(rewards)\n        if self.reputation_system:\n            self.reputation_history.append(self.agent_reputations.copy())\n        \n        # Check if episode is done\n        done = self.round_count >= self.max_rounds\n        \n        # Generate observations\n        observations = []\n        for agent_id in range(self.n_agents):\n            obs = self._get_observation(agent_id)\n            observations.append(obs)\n        \n        # Info dictionary\n        info = {\n            'round_count': self.round_count,\n            'episode_count': self.episode_count,\n            'agent_scores': self.agent_scores.copy(),\n            'last_actions': social_actions,\n            'last_rewards': rewards,\n            'cooperation_rate': self._compute_cooperation_rate(),\n            'average_reputation': np.mean(self.agent_reputations) if self.reputation_system else 0.0,\n        }\n        \n        return observations, rewards, done, info\n    \n    def _get_observation(self, agent_id: int) -> np.ndarray:\n        \"\"\"\n        Get observation for a specific agent.\n        \n        Includes:\n        - Own score\n        - Other agents' reputations (if enabled)\n        - Recent action history\n        - Game-specific state information\n        \"\"\"\n        obs_components = []\n        \n        # Own score (normalized)\n        max_possible_score = self.max_rounds * 10  # Rough estimate\n        own_score_norm = self.agent_scores[agent_id] / max_possible_score\n        obs_components.append(own_score_norm)\n        \n        # Round information\n        round_progress = self.round_count / self.max_rounds\n        obs_components.append(round_progress)\n        \n        # Reputation information\n        if self.reputation_system:\n            # Own reputation as seen by others (average)\n            own_reputation = np.mean([self.agent_reputations[i, agent_id] \n                                    for i in range(self.n_agents) if i != agent_id])\n            obs_components.append(own_reputation)\n            \n            # Other agents' reputations\n            for other_id in range(self.n_agents):\n                if other_id != agent_id:\n                    obs_components.append(self.agent_reputations[agent_id, other_id])\n        \n        # Recent action history (last 3 rounds)\n        history_length = min(3, len(self.action_history))\n        for i in range(history_length):\n            round_actions = self.action_history[-(i+1)]\n            # Own action\n            obs_components.append(float(round_actions[agent_id].value))\n            # Others' actions\n            for other_id in range(self.n_agents):\n                if other_id != agent_id:\n                    obs_components.append(float(round_actions[other_id].value))\n        \n        # Pad with zeros if not enough history\n        for i in range(3 - history_length):\n            obs_components.extend([0.0] * self.n_agents)  # Own + others' actions\n        \n        # Game-specific observations\n        if self.game_type == \"public_goods\":\n            # Add information about current endowment\n            obs_components.append(self.endowment / 10.0)  # Normalized\n        \n        return np.array(obs_components, dtype=np.float32)\n    \n    def _compute_prisoners_dilemma_rewards(self, actions: List[SocialAction]) -> List[float]:\n        \"\"\"Compute rewards for prisoner's dilemma.\"\"\"\n        if self.n_agents != 2:\n            raise ValueError(\"Prisoner's dilemma requires exactly 2 agents\")\n        \n        action_pair = (actions[0], actions[1])\n        if action_pair in self.payoff_matrix:\n            return list(self.payoff_matrix[action_pair])\n        else:\n            # Default to mutual defection\n            return [1.0, 1.0]\n    \n    def _compute_public_goods_rewards(self, actions: List[SocialAction]) -> List[float]:\n        \"\"\"Compute rewards for public goods game.\"\"\"\n        # Count contributions\n        contributions = sum(1 for action in actions if action == SocialAction.COOPERATE)\n        \n        # Total public good\n        public_good = contributions * self.multiplier\n        \n        # Individual rewards\n        rewards = []\n        for i, action in enumerate(actions):\n            reward = public_good / self.n_agents  # Share of public good\n            \n            if action == SocialAction.COOPERATE:\n                reward -= self.contribution_cost  # Cost of contributing\n            \n            rewards.append(reward)\n        \n        return rewards\n    \n    def _compute_resource_sharing_rewards(self, actions: List[SocialAction]) -> List[float]:\n        \"\"\"Compute rewards for resource sharing game.\"\"\"\n        # Count sharers and hoarders\n        sharers = sum(1 for action in actions if action == SocialAction.SHARE)\n        hoarders = sum(1 for action in actions if action == SocialAction.HOARD)\n        \n        # Base allocation\n        base_allocation = self.total_resources / self.n_agents\n        \n        rewards = []\n        for action in actions:\n            if action == SocialAction.SHARE:\n                # Sharing bonus proportional to number of sharers\n                bonus = self.sharing_bonus * (sharers / self.n_agents)\n                reward = base_allocation * (1 + bonus)\n            else:  # HOARD\n                # Hoarding penalty\n                penalty = self.hoarding_penalty * (hoarders / self.n_agents)\n                reward = base_allocation * (1 - penalty)\n            \n            rewards.append(reward)\n        \n        return rewards\n    \n    def _compute_trust_game_rewards(self, actions: List[SocialAction]) -> List[float]:\n        \"\"\"Compute rewards for trust game.\"\"\"\n        if self.n_agents != 2:\n            raise ValueError(\"Trust game requires exactly 2 agents\")\n        \n        trustor_action, trustee_action = actions\n        \n        trustor_reward = self.initial_endowment\n        trustee_reward = self.initial_endowment\n        \n        if trustor_action == SocialAction.COOPERATE:  # Trust\n            # Trustor gives money to trustee\n            trustor_reward = 0\n            trustee_reward = self.initial_endowment * self.trust_multiplier\n            \n            if trustee_action == SocialAction.COOPERATE:  # Trustworthy\n                # Trustee shares the multiplied amount\n                shared_amount = trustee_reward / 2\n                trustor_reward = shared_amount\n                trustee_reward = shared_amount\n        \n        return [trustor_reward, trustee_reward]\n    \n    def _update_reputations(self, actions: List[SocialAction], rewards: List[float]):\n        \"\"\"Update reputation matrix based on actions and outcomes.\"\"\"\n        learning_rate = 0.1\n        \n        for i in range(self.n_agents):\n            for j in range(self.n_agents):\n                if i != j:\n                    # Update i's opinion of j based on j's action\n                    if actions[j] in [SocialAction.COOPERATE, SocialAction.SHARE]:\n                        # Cooperative action increases reputation\n                        self.agent_reputations[i, j] += learning_rate * (1.0 - self.agent_reputations[i, j])\n                    else:\n                        # Defective action decreases reputation\n                        self.agent_reputations[i, j] -= learning_rate * self.agent_reputations[i, j]\n                    \n                    # Clamp to [0, 1]\n                    self.agent_reputations[i, j] = np.clip(self.agent_reputations[i, j], 0.0, 1.0)\n    \n    def _compute_cooperation_rate(self) -> float:\n        \"\"\"Compute cooperation rate over recent history.\"\"\"\n        if not self.action_history:\n            return 0.0\n        \n        recent_rounds = min(5, len(self.action_history))\n        cooperative_actions = 0\n        total_actions = 0\n        \n        for round_actions in self.action_history[-recent_rounds:]:\n            for action in round_actions:\n                if action in [SocialAction.COOPERATE, SocialAction.SHARE]:\n                    cooperative_actions += 1\n                total_actions += 1\n        \n        return cooperative_actions / max(1, total_actions)\n    \n    def render(self, mode: str = 'human'):\n        \"\"\"Render the environment state.\"\"\"\n        if mode == 'human':\n            print(f\"\\n=== Social Environment: {self.game_type} ===\")\n            print(f\"Episode {self.episode_count}, Round {self.round_count}/{self.max_rounds}\")\n            print(f\"Scores: {self.agent_scores}\")\n            \n            if self.reputation_system:\n                print(\"\\nReputation Matrix:\")\n                for i in range(self.n_agents):\n                    row = \" \".join([f\"{self.agent_reputations[i, j]:.2f}\" for j in range(self.n_agents)])\n                    print(f\"Agent {i}: [{row}]\")\n            \n            if self.action_history:\n                print(f\"\\nLast actions: {[action.name for action in self.action_history[-1]]}\")\n                print(f\"Last rewards: {self.outcome_history[-1]}\")\n            \n            print(f\"Cooperation rate: {self._compute_cooperation_rate():.2f}\")\n    \n    def get_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get environment statistics for analysis.\"\"\"\n        stats = {\n            'episode_count': self.episode_count,\n            'round_count': self.round_count,\n            'game_type': self.game_type,\n            'n_agents': self.n_agents,\n            'agent_scores': self.agent_scores.tolist(),\n            'total_score': np.sum(self.agent_scores),\n            'score_variance': np.var(self.agent_scores),\n            'cooperation_rate': self._compute_cooperation_rate(),\n        }\n        \n        if self.reputation_system:\n            stats.update({\n                'average_reputation': np.mean(self.agent_reputations),\n                'reputation_variance': np.var(self.agent_reputations),\n                'reputation_matrix': self.agent_reputations.tolist(),\n            })\n        \n        if self.action_history:\n            # Analyze action patterns\n            action_counts = {action: 0 for action in SocialAction}\n            for round_actions in self.action_history:\n                for action in round_actions:\n                    action_counts[action] += 1\n            \n            stats['action_distribution'] = {\n                action.name: count / (len(self.action_history) * self.n_agents)\n                for action, count in action_counts.items()\n            }\n        \n        return stats\n    \n    def close(self):\n        \"\"\"Close the environment.\"\"\"\n        pass\n\n\nclass TheoryOfMindEnvironment(SocialDilemmaEnvironment):\n    \"\"\"\n    Extended social environment that requires theory of mind reasoning.\n    \n    Agents must model other agents' beliefs, desires, and intentions\n    to succeed in complex social interactions.\n    \"\"\"\n    \n    def __init__(self, \n                 belief_tracking: bool = True,\n                 intention_modeling: bool = True,\n                 **kwargs):\n        \"\"\"\n        Initialize theory of mind environment.\n        \n        Args:\n            belief_tracking: Whether agents should track others' beliefs\n            intention_modeling: Whether agents should model others' intentions\n        \"\"\"\n        self.belief_tracking = belief_tracking\n        self.intention_modeling = intention_modeling\n        \n        super().__init__(**kwargs)\n        \n        # Theory of mind state\n        self.agent_beliefs = {}  # What each agent believes about the world\n        self.agent_intentions = {}  # What each agent intends to do\n        self.belief_history = []\n    \n    def reset(self) -> List[np.ndarray]:\n        \"\"\"Reset with theory of mind state.\"\"\"\n        obs = super().reset()\n        \n        # Initialize beliefs and intentions\n        for agent_id in range(self.n_agents):\n            self.agent_beliefs[agent_id] = {\n                'other_cooperation_probability': np.random.uniform(0.3, 0.7, self.n_agents-1),\n                'game_understanding': np.random.uniform(0.5, 1.0)\n            }\n            self.agent_intentions[agent_id] = {\n                'cooperation_tendency': np.random.uniform(0.3, 0.7),\n                'adaptation_rate': np.random.uniform(0.1, 0.3)\n            }\n        \n        self.belief_history = []\n        return obs\n    \n    def step(self, actions: List[int]) -> Tuple[List[np.ndarray], List[float], bool, Dict]:\n        \"\"\"Step with theory of mind updates.\"\"\"\n        obs, rewards, done, info = super().step(actions)\n        \n        # Update beliefs about other agents\n        if self.belief_tracking:\n            self._update_beliefs(actions, rewards)\n        \n        # Update intentions based on outcomes\n        if self.intention_modeling:\n            self._update_intentions(actions, rewards)\n        \n        # Add theory of mind info\n        info['agent_beliefs'] = self.agent_beliefs.copy()\n        info['agent_intentions'] = self.agent_intentions.copy()\n        \n        return obs, rewards, done, info\n    \n    def _update_beliefs(self, actions: List[int], rewards: List[float]):\n        \"\"\"Update agents' beliefs about others.\"\"\"\n        social_actions = [SocialAction(action) for action in actions]\n        \n        for agent_id in range(self.n_agents):\n            beliefs = self.agent_beliefs[agent_id]\n            \n            # Update cooperation probability beliefs\n            other_idx = 0\n            for other_id in range(self.n_agents):\n                if other_id != agent_id:\n                    was_cooperative = social_actions[other_id] in [SocialAction.COOPERATE, SocialAction.SHARE]\n                    \n                    # Bayesian-like update\n                    current_belief = beliefs['other_cooperation_probability'][other_idx]\n                    learning_rate = 0.2\n                    \n                    if was_cooperative:\n                        beliefs['other_cooperation_probability'][other_idx] += learning_rate * (1.0 - current_belief)\n                    else:\n                        beliefs['other_cooperation_probability'][other_idx] -= learning_rate * current_belief\n                    \n                    # Clamp to [0, 1]\n                    beliefs['other_cooperation_probability'][other_idx] = np.clip(\n                        beliefs['other_cooperation_probability'][other_idx], 0.0, 1.0\n                    )\n                    \n                    other_idx += 1\n    \n    def _update_intentions(self, actions: List[int], rewards: List[float]):\n        \"\"\"Update agents' intentions based on outcomes.\"\"\"\n        for agent_id in range(self.n_agents):\n            intentions = self.agent_intentions[agent_id]\n            \n            # Adapt cooperation tendency based on reward\n            reward = rewards[agent_id]\n            adaptation_rate = intentions['adaptation_rate']\n            \n            if reward > 0:\n                # Positive reward - slightly increase cooperation\n                intentions['cooperation_tendency'] += adaptation_rate * 0.1\n            else:\n                # Poor reward - slightly decrease cooperation\n                intentions['cooperation_tendency'] -= adaptation_rate * 0.1\n            \n            # Clamp to [0, 1]\n            intentions['cooperation_tendency'] = np.clip(\n                intentions['cooperation_tendency'], 0.0, 1.0\n            )\n    \n    def _get_observation(self, agent_id: int) -> np.ndarray:\n        \"\"\"Get observation including theory of mind information.\"\"\"\n        obs = super()._get_observation(agent_id)\n        \n        # Add belief and intention information\n        if self.belief_tracking and agent_id in self.agent_beliefs:\n            beliefs = self.agent_beliefs[agent_id]\n            obs = np.concatenate([\n                obs,\n                beliefs['other_cooperation_probability'],\n                [beliefs['game_understanding']]\n            ])\n        \n        if self.intention_modeling and agent_id in self.agent_intentions:\n            intentions = self.agent_intentions[agent_id]\n            obs = np.concatenate([\n                obs,\n                [intentions['cooperation_tendency']]\n            ])\n        \n        return obs